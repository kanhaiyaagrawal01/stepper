{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1UB0mU-jIecLw0mRE2kGClAJU4rmTke-f",
      "authorship_tag": "ABX9TyMFwDY6Rf6D6IfFVTR+y7m0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanhaiyaagrawal01/stepper/blob/master/Copy_of_Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooQLnucGRWTC"
      },
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BBPrmqNRaPK"
      },
      "source": [
        "mov_averages = [0,2,5,10,15,20,25,30]\n",
        "max_buy = 5\n",
        "max_keep = 20\n",
        "base_amount = 10000\n",
        "min_amount = 4000\n",
        "max_amount = 24000\n",
        "n_days = 230\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "class Stock:\n",
        "    \n",
        "    def __init__(self, symbol, series=None):\n",
        "        self.symbol = symbol\n",
        "        self.history = create_price_series(n_days) if series is None else series\n",
        "        self.states()\n",
        "        self.index = mov_averages[-1]\n",
        "        self.advance()\n",
        "    \n",
        "    def set_price(self, price, date):\n",
        "        self.price = price\n",
        "        self.date = date\n",
        "    \n",
        "    def set_date(self, date):\n",
        "        self.date = date\n",
        "        self.price = self.history[date]\n",
        "        self.index = self.history.index.get_loc(date)\n",
        "        \n",
        "    def advance(self):\n",
        "        self.index += 1\n",
        "        self.set_date(self.history.index[self.index])\n",
        "        \n",
        "    def set_index(self, index):\n",
        "        self.set_date(self.history.index[index])\n",
        "    \n",
        "    def get_state(self):\n",
        "        return self.states[self.index-mov_averages[-1]-1]\n",
        "        \n",
        "    def states(self):\n",
        "        cumsum = np.array(self.history).cumsum()\n",
        "        self.states = []\n",
        "        for i in range(1,len(self.history)-mov_averages[-1]):\n",
        "            state = [self.history[i+mov_averages[-1]]]\n",
        "            for j in range(1, len(mov_averages)):\n",
        "                first = cumsum[i+mov_averages[-1]-1-mov_averages[j]]\n",
        "                last = cumsum[i+mov_averages[-1]-1]\n",
        "                state.append((last-first)/mov_averages[j])\n",
        "            self.states.append(state)\n",
        "        \n",
        "#     def moving_average(self, lookback):\n",
        "#         if self.index < lookback:\n",
        "#             raise Exception('Lookback window too large!!')\n",
        "#         else:\n",
        "#             mean = self.history[self.index-lookback:self.index].mean()\n",
        "# #             print(mean)\n",
        "#             return mean\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.__repr__();\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Stock (Symbol:{self.symbol}, Price:{self.price}, Date:{self.date})'\n",
        "        \n",
        "class Trader():\n",
        "    \n",
        "    def __init__(self, stock, cash, date):\n",
        "        self.stock = stock\n",
        "        self.position = 0\n",
        "        self.cash = cash\n",
        "        self.startCash = cash\n",
        "        self.startDate = date\n",
        "    \n",
        "    def buy(self, x):\n",
        "        # print(\"buy\", x)\n",
        "        self.position += x\n",
        "        self.cash -= x*self.stock.price\n",
        "        # print(self.position, self.get_state())\n",
        "        \n",
        "    def sell(self, x):\n",
        "        self.position -= x\n",
        "        self.cash += x*self.stock.price\n",
        "        \n",
        "    def close(self):\n",
        "        self.sell(self.position)\n",
        "    \n",
        "    def value(self):\n",
        "        return (self.position*self.stock.price) + self.cash\n",
        "    \n",
        "    def returns(self):\n",
        "        return (self.value() - self.startCash)/self.startCash\n",
        "    \n",
        "    def get_state(self):\n",
        "        res = np.zeros(len(mov_averages)+2)\n",
        "        st = np.array(self.stock.get_state())\n",
        "        for i in range(len(mov_averages)):\n",
        "          res[i] = st[i]\n",
        "        res[-2] = self.position\n",
        "        res[-1] = self.returns()\n",
        "        return res\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.__repr__();\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Trader ( {self.stock}, Position:{self.position}, CashValue:{self.cash}, AccountValue:{self.value()}, Returns:{self.returns()*100}%)'\n",
        "    \n",
        "# class Strategy():\n",
        "#     def __init__(self, trader, lookback=30, stopLoss=0.9, stopGain=1.2):\n",
        "#         self.trader = trader\n",
        "#         self.lookback = lookback\n",
        "#         self.stopLoss = stopLoss\n",
        "#         self.stopGain = stopGain\n",
        "        \n",
        "#     def signal(self):\n",
        "#         if self.trader.returns()+1>=self.stopGain:\n",
        "#             return -self.trader.position\n",
        "#         elif self.trader.returns()+1<=self.stopLoss:\n",
        "#             return -self.trader.position\n",
        "#         else:\n",
        "#             try:\n",
        "#                 avg = self.trader.stock.moving_average(self.lookback)\n",
        "#                 if self.trader.stock.price > avg:\n",
        "#                     return -1\n",
        "#                 else:\n",
        "#                     return 1\n",
        "#             except Exception as e:\n",
        "# #                 print(e)\n",
        "#                 return 0\n",
        "            \n",
        "#     def run(self, startDate, endDate):\n",
        "#         self.trader.stock.set_date(startDate)\n",
        "#         while True:\n",
        "#             sig = self.signal()\n",
        "#             self.trader.buy(sig)\n",
        "# #             print(str(trader))\n",
        "#             if self.trader.stock.date == endDate:\n",
        "#                 break\n",
        "#             self.trader.stock.advance()\n",
        "            \n",
        "\n",
        "def create_price_series(length, startdate=dt.datetime.now(), initial_price = 100):\n",
        "    index = np.array(startdate, dtype=np.datetime64) + pd.to_timedelta(np.arange(length), 'D')\n",
        "    prices = [initial_price]\n",
        "    for _ in range(1, length):\n",
        "        prices.append(prices[-1] + 0.1*initial_price*(random.random()-0.5))\n",
        "    return pd.Series(prices, index=index)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkRwkcT9xmnq"
      },
      "source": [
        "class TradingEnv(Env):\n",
        "    def __init__(self):\n",
        "        self.action_space = Discrete(2*max_buy + 1)\n",
        "        self.observation_space = Box(low=-max_keep, high=200, dtype=np.float64, shape=(10,))\n",
        "        # self.observation_space = Box(low=np.array([0]*(len(mov_averages))+[-max_keep,min_amount]), high=np.array([200]*(len(mov_averages))+[max_keep, max_amount]))\n",
        "        \n",
        "        stock = Stock('AAPL')\n",
        "        self.trader = Trader(stock, base_amount, dt.datetime.now())\n",
        "        self.state = self.trader.get_state()\n",
        "    \n",
        "    def step(self, action):\n",
        "        #state\n",
        "        self.trader.buy(action-max_buy)\n",
        "        # print(\"step1\", self.trader.position)\n",
        "        # print(\"step2\", self.trader.get_state())\n",
        "        self.trader.stock.advance()\n",
        "        # print(\"step3\", self.trader.position)\n",
        "        # print(\"step4\", self.trader.get_state())\n",
        "        self.state = self.trader.get_state()\n",
        "        \n",
        "        #reward\n",
        "        reward = self.trader.value()-base_amount\n",
        "        \n",
        "        #done\n",
        "        if self.trader.stock.index>=n_days-1:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "        \n",
        "        #info\n",
        "        info = {str(self.trader)}\n",
        "        \n",
        "        return self.state, reward, done, info\n",
        "    \n",
        "    def render(self):\n",
        "        pass\n",
        "    def reset(self):\n",
        "        stock = Stock('AAPL')\n",
        "        self.trader = Trader(stock, base_amount, dt.datetime.now())\n",
        "        self.state = self.trader.get_state()\n",
        "        return self.state"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZgN06EN504z",
        "outputId": "dca2f01b-6ba1-43f3-f459-697b60f9fcf5"
      },
      "source": [
        "[0]*(len(mov_averages))+[-max_keep,min_amount]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, -20, 4000]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl-ygZGeRnYr",
        "outputId": "60ff1281-aca7-4d96-a2c8-cc112132e980"
      },
      "source": [
        "env = TradingEnv()\n",
        "episodes = 10\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    info = {}\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        score = reward\n",
        "        # print(state, state.shape)\n",
        "    print('Episode={} Score={} Trader={} state'.format(episode, score, state), state.shape, env.trader.value())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode=1 Score=-255.51191377454234 Trader=[ 1.12492518e+02  1.06633856e+02  1.04382014e+02  1.00945656e+02\n",
            "  9.95671797e+01  1.00292216e+02  1.01621262e+02  1.02504994e+02\n",
            "  5.00000000e+00 -2.55511914e-02] state (10,) 9744.488086225458\n",
            "Episode=2 Score=-1754.8031919722198 Trader=[50.19166425 50.09741913 50.6802274  48.90599693 47.06255113 44.12494172\n",
            " 43.55780436 45.03131203 74.         -0.17548032] state (10,) 8245.19680802778\n",
            "Episode=3 Score=349.7016783463732 Trader=[1.06948681e+02 1.08660742e+02 1.10784607e+02 1.14070136e+02\n",
            " 1.14296671e+02 1.13273030e+02 1.11459388e+02 1.10041690e+02\n",
            " 7.10000000e+01 3.49701678e-02] state (10,) 10349.701678346373\n",
            "Episode=4 Score=711.2793269181748 Trader=[9.57712929e+01 9.86409165e+01 9.93241898e+01 9.64798303e+01\n",
            " 9.49384573e+01 9.44039128e+01 9.35914469e+01 9.17967358e+01\n",
            " 3.80000000e+01 7.11279327e-02] state (10,) 10711.279326918175\n",
            "Episode=5 Score=948.558949399081 Trader=[88.57716363 85.21378483 87.57732976 87.57544962 85.28596076 83.20484462\n",
            " 82.02076078 81.12081454 -4.          0.09485589] state (10,) 10948.558949399081\n",
            "Episode=6 Score=18.141124400051922 Trader=[ 1.06083388e+02  1.10487482e+02  1.09336435e+02  1.06152114e+02\n",
            "  1.02833375e+02  9.83543604e+01  9.57815364e+01  9.43576451e+01\n",
            " -9.40000000e+01  1.81411244e-03] state (10,) 10018.141124400052\n",
            "Episode=7 Score=-47.45126163765781 Trader=[ 1.13514446e+02  1.17150682e+02  1.13150785e+02  1.10649028e+02\n",
            "  1.08942611e+02  1.09755071e+02  1.13114814e+02  1.15060500e+02\n",
            "  1.60000000e+01 -4.74512616e-03] state (10,) 9952.548738362342\n",
            "Episode=8 Score=1121.2829768773772 Trader=[91.61062182 92.73466166 92.96614544 90.14292543 88.73745399 90.66602297\n",
            " 91.92204858 91.75480244 83.          0.1121283 ] state (10,) 11121.282976877377\n",
            "Episode=9 Score=-379.7658055639713 Trader=[ 1.17838715e+02  1.20632423e+02  1.17979364e+02  1.16363877e+02\n",
            "  1.15238991e+02  1.14141206e+02  1.14950472e+02  1.15269178e+02\n",
            "  1.20000000e+01 -3.79765806e-02] state (10,) 9620.234194436029\n",
            "Episode=10 Score=24.270371689472086 Trader=[1.17498304e+02 1.15895848e+02 1.15121563e+02 1.15285547e+02\n",
            " 1.14623733e+02 1.13021739e+02 1.11701243e+02 1.10636299e+02\n",
            " 4.70000000e+01 2.42703717e-03] state (10,) 10024.270371689472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYTH1h6IRp1r"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i-sR8cKRs_C",
        "outputId": "a5bc97de-353b-459e-9bc1-e901c9c1909e"
      },
      "source": [
        "states = env.observation_space.shape\n",
        "actions = env.action_space.n\n",
        "states"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "274J3Z5CRvsf"
      },
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, activation='relu', input_shape=(10,)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmQoChl8Rxik",
        "outputId": "a2c15afb-434f-4f11-8f6b-24322d567822"
      },
      "source": [
        "del model\n",
        "model = build_model(states, actions)\n",
        "model.summary()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 24)                264       \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 11)                275       \n",
            "=================================================================\n",
            "Total params: 1,139\n",
            "Trainable params: 1,139\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5KmGhM7RzOh"
      },
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBH4MxE4R2Up"
      },
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = BoltzmannQPolicy()\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "4LgSy2O9R-Pa",
        "outputId": "e9f0f381-1a6a-4f0f-86ec-1b154e2ce0ab"
      },
      "source": [
        "dqn = build_agent(model, actions) \n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-69f3cdb8cfd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     inputs, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1200\u001b[0;31m         x, extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1201\u001b[0m     \u001b[0;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[0;31m# at this point.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2334\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2361\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    529\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_21_input to have 2 dimensions, but got array with shape (1, 1, 10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fe90bjqR_4p"
      },
      "source": [
        "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4PDtOnNzHAG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}